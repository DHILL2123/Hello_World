{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c9faf4",
   "metadata": {},
   "source": [
    "# Module Goals\n",
    "\n",
    "## Acquisition\n",
    "\n",
    "* Acquire structured data from SQL to Pandas\n",
    "\n",
    "* Summarize Summarize the data through aggregates, descriptive stats and distribution plots (histograms, density plots, boxplots, e.g.). (pandas: .value_counts, .head, .shape, .describe, .info, matplotlib.pyplot.hist, seaborn.boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab952e",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "* Clean the data by converting datatypes and handle missing values. (pandas: .isnull, .value_counts, .dropna, .replace)\n",
    "\n",
    "* Split our observations into 3 samples, Train, Validate, and Test. (sklearn.model_selection.train_test_split).\n",
    "\n",
    "* Scale our numeric data so that all variables are on the same scale, such as between 0 and 1. We will discuss the importance of \"scaling\", different methods for scaling data, and why to use one type over another. (sklearn.preprocessing: StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578ed1d",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "* Hypothesize: We will discuss the meaning of \"drivers\", variables vs. features, and the target variable. We will discuss the importance of documenting questions and hypotheses, obtaining answers for those questions, and documenting takeaways and findings at each step of exploration.\n",
    "\n",
    "* Visualize the interaction of variables, especially independent variables with the dependent variable using charts such as scatterplots, jointplots, pairgrids, and heatmaps to identify drivers.\n",
    "\n",
    "* Test Hypotheses that involve a continuous variable using t-tests and correlation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b4717",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "* Feature Engineering: We will learn ways to identify, select, and create features through feature importance. We will discuss the \"Curse of Dimensionality.\" (sklearn.feature_selection.f_regression).\n",
    "\n",
    "* Establish Baseline: We will learn about the importance of establishing a \"baseline model\" or baseline score and ways to complete this task. The baseline for regression is often computed by predicting each observation's value to be the mean or median of the dependent variable.\n",
    "\n",
    "* Build Models: We will build linear regression models. What does that mean? We will extract the patterns in the data using well established algorithms, so that we don't have to do that manually. An example of a regression algorithm is the glm (generalized linear model). The algorithm will return to us a mathematical model or function (e.g. y = 3x + 2). That function will be used to compute predictions for each observation. We will learn about the differences in the most common regression algorithms. (sklearn.linear_model)\n",
    "\n",
    "* Model Evaluation: We will compare regression models by computing evaluation metrics, i.e. metrics that measure how well a model did at predicting the target variable. (statsmodels.formula.api.ols, sklearn.metrics, math.sqrt)\n",
    "\n",
    "* Model Selection and Testing: We will learn how to select a model, and we will test the model on the unseen data sample (the out-of-sample data in the validate and then test datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9904d",
   "metadata": {},
   "source": [
    "# Building a Data Product\n",
    "\n",
    "We will end with an end-to-end project practicing steps of the data science pipeline from planning through model selection and delivery.\n",
    "\n",
    "# About Regression\n",
    "\n",
    "Regression is a supervised machine learning technique used to model the relationship of one or more features or independent variables, (one = simple regression, more = multiple regression) to one or more target or dependent variables, (one = univariate regression, more = multivariate regression). The variables are represented by continuous data.\n",
    "\n",
    "A regression algorithm attempts to find the function that best 'mimics' or 'models' the relationship between independent feature(s) and dependent target variable(s). The algorithm does this by finding the line (for simple regression) or plane (for multiple regression) that minimizes the errors in our predictions when compared to the labeled data. Once we acquire that function, it can be used to make predictions on new observations when they become available; we can simply run these new values of the independent variable(s) through the function for each observation to predict the dependent target variable(s).\n",
    "\n",
    "The algorithm attempts to find the “best” choices of values for the parameters, which in a linear regression model are the coefficients, \n",
    "b\n",
    "i\n",
    ", in order to make the formula as “accurate” as possible, i.e. minimize the error. There are different ways to define the error, but whichever evaluation metric we select, the algorithm finds the line of best fit by identifying the parameters that minimize that error.\n",
    "\n",
    "Once estimated, the parameters (intercept and coefficients) allow the value of the target variable to be obtained from the values of the feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b9af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
